{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4G10 Coursework 2: predicting hand kinematics from neural data\n",
    "\n",
    "\n",
    "Please read carefully the last section of this notebook, which gives some of our expectations regarding your report.\n",
    "\n",
    "In this handout, \n",
    "- <u>text that is underlined</u> corresponds to things you have to do / implement.\n",
    "- **text in bold** corresponds to questions you need to answer in some form in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T23:15:23.817600200Z",
     "start_time": "2023-12-09T23:15:23.682077Z"
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import scipy.stats\n",
    "from scipy import signal\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "In this piece of 4G10 coursework, you will use neural data recorded in the primary motor cortex (M1) of a reaching monkey to predict the kinematics of the monkey's hand.\n",
    "\n",
    "The monkey initiated each trial by placing their hand in the center of a fronto-parallel screen. A target then appeared on the screen. The monkey had to wait for a ‘go’ cue before making a reaching movement towards the instructed target. The targets were placed in various positions in a virtual maze, which changed in each trial, forcing the monkey to make a variety of reaching movements across trials.\n",
    "\n",
    "The activity of $N=162$ motor cortical neurons was recorded simultaneously, alongside the kinematics of the animal's hand.\n",
    "\n",
    "In the dataset presented below, all time series are partitioned into trials. Each trial begins at the go cue and lasts 800ms ($T = 16$ bins of 50ms duration) — roughly the duration of a reach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T23:15:23.887612900Z",
     "start_time": "2023-12-09T23:15:23.705581100Z"
    }
   },
   "outputs": [],
   "source": [
    "# grab the data from the server\n",
    "r = requests.get('https://4G10.cbl-cambridge.org/data.npz', stream = True)\n",
    "data = np.load(BytesIO(r.raw.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among other things (detailed later), this dictionary numerical arrays indexed by the following keys:\n",
    "- `\"hand_train\" (2 × 400 × T)`: 2D velocity (X/Y) of the monkey's hand in 400 ‘train’ trials;\n",
    "- `\"neural_train\" (N × 400 × T)`: neural activity (spike counts) in the same 400 ‘train’ trials;\n",
    "- `\"neural_test\" (N × 100 × T)`: neural activity (spike counts) in 100 ‘test’ trials.\n",
    "\n",
    "E.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Visualisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def plot_psths(data, timeintervals, n_rows, n_cols, n_cond):\n",
    "    np.random.seed(41)\n",
    "    cond = np.random.randint(low=data.shape[1], size=n_cond)\n",
    "\n",
    "    fig, ax = plt.subplots(n_rows, n_cols, tight_layout=True, figsize=(8,12))\n",
    "    for row in ax:\n",
    "        n = np.random.randint(low=data.shape[0], size=1)[0]\n",
    "        for c in cond:\n",
    "            row.plot(timeintervals, data[n, c,:], label = 'Trial = '+str(c))\n",
    "            row.set_title(f'Neuron {n}')\n",
    "            row.set_xlabel('Time relative to onset of hand movement (ms)')\n",
    "            row.set_ylabel('Trial-averaged spike rate (Hz)')\n",
    "            row.legend(loc='upper left', prop={'size': 10})\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T23:15:23.911119100Z",
     "start_time": "2023-12-09T23:15:23.854606900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this CW is to implement some of the modelling / decoding techniques you have been taught in lectures, to predict the monkey's 2D hand velocity in the 100 test trials for which you are only given neural activity. Your predictions will be based on the training data provided (`hand_train, neural_train`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Baseline decoder: simple Gaussian smoothing + linear regression\n",
    "\n",
    "To establish a meaningful baseline, you will first implement a very simple two-stage decoder.\n",
    "In the first stage, you will smooth the spike count time series of each neuron by convolving it with a Gaussian filter of width $\\sigma$; in continuous time, such a Gaussian filter is given by $f(t) \\propto \\exp(-t^2/2\\sigma^2)$.\n",
    "In the second stage, you will use ridge regression to learn an instantaneous linear decoder given by\n",
    "\n",
    "$$ \\hat{v}_{k,t} = W \\tilde{x}_{k,t} $$\n",
    "\n",
    "where $\\hat{v}_{k, t} \\in \\mathbb{R}^2$ is the predicted velocity of the hand in test trial $k$ and time bin $t$, $\\tilde{x}_{k, t} \\in \\mathbb{R}^N$ is the $t^\\text{th}$ time bin of the temporally smoothed spike counts in test trial $k$, and $W$ is a `2 × N` matrix of decoding weights. Note that the hand velocity data has been centered already, so there is no need to include a bias term in the regression.\n",
    "\n",
    "The optimal ridge regression weights are given by\n",
    "$$  W^\\star = V \\tilde{X}^\\top (\\tilde{X} \\tilde{X}^\\top + \\lambda I_N )^{-1} $$\n",
    "where $V$ is the $2 × (400*16)$ matrix of hand velocities from the training set (with all trials and time bins concatenated horizontally), and similarly $\\tilde{X}$ is the $N × (400*16)$ matrix of smoothed neural spike counts in the training set.\n",
    "\n",
    "In the equation above, $\\lambda$ is a regularisation parameter which helps protect against overfitting.\n",
    "The choice of value for this parameter is left up to you, so long as you can provide a justification (there are several sensible possibilities).\n",
    "\n",
    "The goal here is to make the best possible predictions you can of the held out hand velocity data in test trials, based on the neural activity in the same trials. When you are ready to test your predictions, you can submit them as a 3D numpy array of shape `2 × 100 × 16` to https://4G10.cbl-cambridge.org. If you get a “HTTP error 400” back, it probably means the format is wrong. Your numpy array must be saved using the `np.save(\"filename.npy\", my_array)` function; the server also expects the array to be of `float64` numerical type — this should be the default in numpy, but if in doubt you can always cast using `my_array.as_type(\"float64\")`. When you submit, please indicate your candidate number and choose \"Simple Gaussian smoothing\" in the dropdown list. Upon uploading, you will receive immediate feedback in the form of an $R^2$ coefficient. The closer to 1, the better!\n",
    "\n",
    "- <u>Implement Gaussian temporal smoothing + ridge regression as outlined above</u>.\n",
    "- **How does the quality of hand velocity predictions vary with the smoothing window length $\\sigma$? How do you interpret that?** You might want to experiment with values between 20 and 80 ms.\n",
    "- **Comment on the suitability of this simple decoding strategy for online (“on the fly”) decoding of movement in a BMI context (consider e.g. feasability, computational tractability, and accuracy). Can you think of a small modification to the above approach that would improve applicability to online decoding?** (bonus points for  implementing it!)\n",
    "- The hand velocity data provided in `data[\"hand_train\"]` had actually been shifted backward by 120ms relative to the neural data (and similarly for the test set, which was not given to you). **Can you speculate about why we did that**?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gaussian Smoothing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def smoother(Neural_train, std):\n",
    "    return gaussian_filter(Neural_train, sigma=std, axes=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T23:15:23.912117100Z",
     "start_time": "2023-12-09T23:15:23.864608800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def W_ML_calc(V, X_til, lam):\n",
    "    lam_I_n = lam * np.identity(X_til.shape[0])\n",
    "    brackets = scipy.linalg.inv(np.tensordot(X_til, X_til, axes=([1,2], [1,2])) + lam_I_n)\n",
    "    return np.tensordot(V, X_til, axes=([1,2], [1,2])) @ brackets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T23:15:23.912117100Z",
     "start_time": "2023-12-09T23:15:23.877112Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "Neural_test = data['neural_test']\n",
    "Neural_train = data['neural_train']\n",
    "Hand_train = data['hand_train']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T23:15:23.995631200Z",
     "start_time": "2023-12-09T23:15:23.894113500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100, 16)\n"
     ]
    }
   ],
   "source": [
    "stds = np.linspace(0.1, 1.2, 5)\n",
    "\n",
    "for std in [1.2]:\n",
    "    Neural_train_filt = smoother(Neural_train, std)\n",
    "    Neural_test_filt = smoother(Neural_test, std)\n",
    "    for lam in [1]:\n",
    "        W_ML = W_ML_calc(Hand_train, Neural_train, lam)\n",
    "        Predicted_vel = np.tensordot(W_ML, Neural_test_filt, axes=([1], [0]))\n",
    "\n",
    "        np.save('Simple_Results/Ex_2_Pred_Vel_lam' + str(lam) + '_std' + str(std) + '.npy', Predicted_vel)\n",
    "\n",
    "print(Predicted_vel.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T23:15:24.266178300Z",
     "start_time": "2023-12-09T23:15:23.959125400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kalman filter-based decoding\n",
    "\n",
    "We now turn to a more sophisticated decoder based on a Kalman filter/smoother.\n",
    "\n",
    "### 3.1 An autoregressive prior for hand kinematics\n",
    "\n",
    "A 10-dimensional linear latent dynamical system (“LDS”; cf lecture notes) was pre-trained for you on the hand velocity data in the training set; specifically, we consider the following generative model:\n",
    "\n",
    "$$\n",
    "(1) \\qquad z_{k, 0} \\sim \\mathcal{N}(\\mu_0, \\Sigma_0) \\\\ \n",
    "(2) \\qquad z_{k, t+1} = A z_{k, t} + \\epsilon_{k, t+1} \\quad \\text{with } \\epsilon_{k, t+1} \\sim \\mathcal{N}(0, Q) \\\\\n",
    "(3) \\qquad v_{k, t} = C z_{k, t} + \\eta_{k, t} \\quad \\text{with } \\eta_{k, t} \\sim \\mathcal{N}(0, R)\n",
    "$$\n",
    "\n",
    "where $z_{k, t} \\in \\mathbb{R}^{10}$ is the latent state in time bin $t$ of trial $k$, and $v_{k, t} \\in \\mathbb{R}^2$ is the corresponding hand velocity. \n",
    "\n",
    "The parameters of this LDS can be found in the same `data` dictionary as above, with the following keys:\n",
    "- \"hand_KF_A\" (`10 × 10`): state matrix $A$\n",
    "- \"hand_KF_C\" (`2 × 10`): output matrix $C$\n",
    "- \"hand_KF_mu0\" (`10 × 1`): initial prior mean $\\mu_0$\n",
    "- \"hand_KF_Sigma0\" (`10 × 10`): initial prior covariance $\\Sigma_0$\n",
    "- \"hand_KF_Q\" (`10 × 10`): process noise covariance matrix $Q$\n",
    "- \"hand_KF_R\" (`2 × 2`): observation noise covariance matrix $R$\n",
    "\n",
    "<u>Write your own Kalman smoother implementation and use it to compute the mean $\\hat{z}_{k, 1:T}$ of the smoothing distribution $p(z_{k, t} | v_{k, 1:T})$, for each trial $k$ in the training set.</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Building an LDS model of neural data using supervised learning\n",
    "\n",
    "Conceptually, the latents $z_{k, 1:T}$ introduced above contain signals related to the velocity of the hand, its acceleration, and potentially higher-order derivatives too — all signals which we have good reasons to suspect that neural activity in M1 is strongly related to. Eqs (1) and (2) above provide a good autoregressive prior model for the temporal dynamics of these signals, and you are now going to use this prior in a generative LDS model of _neural data_, substituting the hand-related likelihood (Eq 3) with a neural likelihood:\n",
    "\n",
    "$$\n",
    "(4) \\qquad x_{k, t} = D z_{k, t} + \\xi_{k, t} \\quad \\text{with } \\xi_{k, t} \\sim \\mathcal{N}(0, S)\n",
    "$$\n",
    "\n",
    "where $x_{k, t}$ denotes neural spike counts in the $t^\\text{th}$ time bin of trial $k$.\n",
    "\n",
    "The combination of Eqs (1), (2) and (4) forms an LDS model which you will be able to invert using Kalman filtering to obtain a filtered posterior $p(z_{k', t} | x_{k', 0:t})$ for any test trial $k'$. From there, you will use Eq. (3) to obtain a filtered predictive distribution for the hand velocity in each test trial, $p(v_{k', t} | x_{k', 0:t})$.\n",
    "\n",
    "- <u>Begin by centering the neural data (both training and testing sets) by removing, for each neuron, its mean activity across both trials and time in the training set.</u>\n",
    "- <u>Fit the likelihood parameters $D$ and $S$ through supervised learning, by maximizing the joint log-likelihood $\\log p(\\hat{z}_{k, 1:T}, x_{k, 1:T})$ averaged over all trials in the training set, where $\\hat{z}_{k,1:T}$ is the posterior mean you obtained in Section 3.1.</u>\n",
    "To do this, write down the average joint log likelihood and **show that it is maximized by the following parameter settings**:\n",
    "    - $ \\displaystyle D^\\star = \\left(\\sum_{k, t} x_{k, t} \\hat{z}_{k, t}^\\top \\right) \\left( \\sum_{k, t} \\hat{z}_{k, t} \\hat{z}_{k, t}^\\top \\right)^{-1} $\n",
    "    - $ \\displaystyle S^\\star = \\frac{1}{KT} \\left( \\sum_{k, t} x_{k, t} x_{k, t}^\\top - D^\\star \\sum_{k, t} \\hat{z}_{k, t} x_{k, t}^\\top \\right) $ where $K$ is the number of trials in the training set. **Include your derivations in your report.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Using Kalman filtering to predict the hand velocity\n",
    "\n",
    "- Based on the model obtained in Section 3.2, <u>write your own Kalman filter implementation and compute the filtered posterior $p(z_{k', t} | x_{k', 0:t}) $ for each trial $k'$ in the test set.</u> What we are really interested in is the mean $\\bar{z}_{k', t}$ of this filtered posterior, as our best prediction of the momentary hand velocity $v_{k', t}$ is then given by $\\hat{v}_{k', t} = C \\bar{z}_{k', t}$.\n",
    "- <u>Submit your predictions to https://4G10.cbl-cambridge.org, in the same format as described in Section 2. Please select \"Kalman filtering\" in the dropdown list. Once again, you will receive immediate feedback in the form of an $R^2$ coefficient.</u> **Include this result in your report, and discuss; in particular, why do you think these predictions are much better than those of Section 2?**\n",
    "- As in Section 2, **comment on the suitability of this more sophisticated decoding strategy for online decoding of movement in a BCI context**.\n",
    "- Finally, **what approach(es) would you suggest to improve decoding performance even further? Include a discussion of the tradeoffs that would arise with these alternative approaches.** (max 1 page in your report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing up\n",
    "\n",
    "\n",
    "Please write up your findings in a report to be submitted on Moodle in PDF format, and **include all your code in the Appendix**. Please clearly include your candidate number, NOT your name, on the front page. Your report should address all the questions raised in this notebook, be structured around the Sections of this notebook, and **be a maximum of five A4 pages** excluding any Appendix (minimum font size 11pt, minimum margins 1.5cm on each side).\n",
    "\n",
    "You are very much encouraged to think of data/results visualisations to best support the exposition of your results. You are also encouraged to report on any specific problems/difficulties that arose in your implementation of the various algorithms, and how you addressed those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T23:15:24.277180200Z",
     "start_time": "2023-12-09T23:15:24.267178600Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
